import pandas as pd
import numpy as np
import re
import nltk
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import TfidfVectorizer
from xgboost import XGBClassifier
from sklearn.metrics import accuracy_score, precision_recall_fscore_support

# Download necessary NLTK data
nltk.download('punkt')
nltk.download('stopwords')

# Load dataset
df = pd.read_csv('/kaggle/input/merged-file-no-duplicates2/merged_file_no_duplicates2.csv')
df.dropna(subset=['text'], inplace=True)

# Text cleaning function
def clean_text(text):
    text = re.sub(r"http\S+|www\S+|https\S+", '', text)
    text = re.sub(r'@\w+|#', '', text)
    text = re.sub(r'[^A-Za-z\s]', '', text).lower()
    return re.sub(r'\s+', ' ', text).strip()

stop_words = set(stopwords.words('english'))

# Clean and tokenize
df['cleaned'] = df['text'].apply(clean_text)
df['tokens'] = df['cleaned'].apply(lambda x: [word for word in word_tokenize(x) if word not in stop_words])

# Prepare data for TF-IDF
df['joined_tokens'] = df['tokens'].apply(lambda x: ' '.join(x))  # TF-IDF needs string input
X = df['joined_tokens']
y = df['label']

# Split the data
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)

# TF-IDF Vectorization
tfidf = TfidfVectorizer(max_features=5000)  # Limit features for speed
xv_train = tfidf.fit_transform(X_train)
xv_test = tfidf.transform(X_test)

# XGBoost model
xgb = XGBClassifier(use_label_encoder=False, eval_metric='logloss', n_jobs=-1)
xgb.fit(xv_train, y_train)

# Predictions and evaluation
y_pred = xgb.predict(xv_test)
acc = accuracy_score(y_test, y_pred)
prec, rec, f1, _ = precision_recall_fscore_support(y_test, y_pred, average='weighted')

# Output results
results_df = pd.DataFrame([{
    'Embedding': 'TF-IDF',
    'Model': 'XGBoost',
    'Accuracy': acc,
    'Precision': prec,
    'Recall': rec,
    'F1 Score': f1
}])

print(results_df)
